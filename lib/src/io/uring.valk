
header "sys" as sys

fn io_uring_prep_rw(op: i32, sqe: sys:io_uring_sqe, fd: i32, addr: ?ptr, len: u32, offset: u64) {
    sqe.opcode = op.to(u8)
    sqe.flags = 0
    sqe.ioprio = 0
    sqe.fd = fd
    sqe.off = offset
    sqe.addr = addr.@cast(u64)
    sqe.len = len
    sqe.event_flags = 0
    // sqe.user_data = 0
    sqe.buf_index = 0
    sqe.personality = 0
    sqe.file_index = 0
    sqe.pad2 = { 0 ... }
}

struct kernel_timespec {
    tv_sec: i64
    tv_nsec: i64
}

fn io_uring_prep_read(sqe: sys:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, offset: u64) {
    io_uring_prep_rw(sys:IORING_OP.READ, sqe, fd, buf, nbytes, offset)
}

fn io_uring_prep_write(sqe: sys:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, offset: u64) {
    io_uring_prep_rw(sys:IORING_OP.WRITE, sqe, fd, buf, nbytes, offset)
}

fn io_uring_prep_poll_add(sqe: sys:io_uring_sqe, fd: i32, poll_mask: u32) {
    io_uring_prep_rw(sys:IORING_OP.POLL_ADD, sqe, fd, null, 0, 0)
    sqe.event_flags = io_uring_prep_poll_mask(poll_mask)
}
fn io_uring_prep_timeout(sqe: sys:io_uring_sqe, ts: kernel_timespec, count: u32, flags: u32) {
    io_uring_prep_rw(sys:IORING_OP.TIMEOUT, sqe, -1, ts, 1, count)
    sqe.event_flags = flags
}

fn io_uring_prep_poll_mask(poll_mask: u32) u32 {
    // TODO:
// #if __BYTE_ORDER == __BIG_ENDIAN
        // poll_mask = __swahw32(poll_mask);
// #endif
    return poll_mask
}


fn io_uring_wait_cqe_nr(ring: sys:io_uring, cqe_ptr: *?sys:io_uring_cqe, wait_nr: u32) i32 {
     return sys:__io_uring_get_cqe(ring, cqe_ptr, 0, wait_nr, null)
}
fn io_uring_wait_cqe(ring: sys:io_uring, cqe_ptr: *?sys:io_uring_cqe) i32 {
    return io_uring_wait_cqe_nr(ring, cqe_ptr, 1)
}
fn io_uring_cqe_seen(ring: sys:io_uring, cqe: ?sys:io_uring_cqe) {
    if isset(cqe) : io_uring_cq_advance(ring, 1);
}
// check if an io_uring completion event is available
fn io_uring_peek_cqe(ring: sys:io_uring, cqe_ptr: *?sys:io_uring_cqe) i32 {
    return io_uring_wait_cqe_nr(ring, cqe_ptr, 0)
}

fn io_uring_cq_advance(ring: sys:io_uring, nr: u32) {
    if nr > 0 {
        let cq = @ref(ring.cq)
        cq.khead[0] = cq.khead[0] + nr
        // atomic(cq.khead[0] + nr)
        // atomic_store_explicit((_Atomic typeof(*(p)) *)(p), (v), memory_order_release)
        // TODO: use atomic_store(cq.khead[0], cq.khead[0] + nr)
    }
}

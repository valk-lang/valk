

#if OS == linux
use ext

use coro

global uring_instance : ?ext:io_uring
global uring_unsubmitted : uint

value IORING_CQE_F_MORE (2)

fn uring() ext:io_uring {
    let ring = uring_instance
    if isset(ring) : return ring
    let r  = ext:io_uring{}
    let err = ext:io_uring_queue_init(1024, r, 0)
    // TODO: check error
    uring_instance = r
    return r
}

fn uring_submit(ring: ext:io_uring) uint {
    let submitc = ext:io_uring_submit(ring)
    uring_unsubmitted -= submitc
    return submitc
}


fn sqe(coro: ?coro:Coro, ring: ?ext:io_uring (null)) ext:io_uring_sqe !full {
    if !isset(coro) : coro = coro:current_coro
    if !isset(ring) : ring = uring()
    let sqe = ext:io_uring_get_sqe(ring)
    if !isset(sqe) {
        uring_submit(ring)
        sqe = ext:io_uring_get_sqe(ring)
    }
    if !isset(sqe) : throw full
    // Set coro
    sqe.user_data = coro.@cast(u64)
    // Submit after x events
    uring_unsubmitted++
    if uring_unsubmitted > 128 : uring_submit(ring)
    //
    return sqe
}


fn io_uring_prep_rw(op: i32, sqe: ext:io_uring_sqe, fd: i32, addr: ?ptr, len: u32, offset: u64) {
    sqe.opcode = op.to(u8)
    sqe.flags = 0
    sqe.ioprio = 0
    sqe.fd = fd
    sqe.off = offset
    sqe.addr = addr.@cast(u64)
    sqe.len = len
    sqe.rw_flags = 0
    // sqe.user_data = 0
    sqe.buf_index = 0
    sqe.personality = 0
    sqe.file_index = 0
    sqe.pad2 = { 0 ... }
}

struct kernel_timespec {
    tv_sec: i64
    tv_nsec: i64
}
fn io_uring_prep_msg_ring(sqe: ext:io_uring_sqe, fd: FD, coro: coro:Coro) {
    io_uring_prep_rw(ext:IORING_OP.MSG_RING, sqe, fd, null, 0, coro.@cast(u64))
    sqe.flags = 1 << ext:IORING_SQE_FLAGS.CQE_SKIP_SUCCESS_BIT
}
fn io_uring_prep_accept(sqe: ext:io_uring_sqe, fd: i32, addr: ?ptr, addrlen: uint, flags: i32) {
    io_uring_prep_rw(ext:IORING_OP.ACCEPT, sqe, fd, addr, 0, addrlen)
    sqe.rw_flags = flags
}
fn io_uring_prep_connect(sqe: ext:io_uring_sqe, fd: i32, addr: ext:libc_sockaddr, addrlen: uint) {
    io_uring_prep_rw(ext:IORING_OP.CONNECT, sqe, fd, addr, 0, addrlen)
}

fn io_uring_prep_read(sqe: ext:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, offset: u64) {
    io_uring_prep_rw(ext:IORING_OP.READ, sqe, fd, buf, nbytes, offset)
}
fn io_uring_prep_recv(sqe: ext:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, flags: i32) {
    io_uring_prep_rw(ext:IORING_OP.RECV, sqe, fd, buf, nbytes, 0)
    sqe.rw_flags = flags
}

fn io_uring_prep_write(sqe: ext:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, offset: u64) {
    io_uring_prep_rw(ext:IORING_OP.WRITE, sqe, fd, buf, nbytes, offset)
}
fn io_uring_prep_send(sqe: ext:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, flags: i32) {
    io_uring_prep_rw(ext:IORING_OP.SEND, sqe, fd, buf, nbytes, 0)
    sqe.rw_flags = flags
}

fn io_uring_prep_poll_add(sqe: ext:io_uring_sqe, fd: i32, poll_mask: u32) {
    io_uring_prep_rw(ext:IORING_OP.POLL_ADD, sqe, fd, null, 0, 0)
    sqe.rw_flags = io_uring_prep_poll_mask(poll_mask)
}
fn io_uring_prep_poll_multi(sqe: ext:io_uring_sqe, fd: i32, poll_mask: u32) {
    io_uring_prep_rw(ext:IORING_OP.POLL_ADD, sqe, fd, null, 1, 0)
    sqe.rw_flags = io_uring_prep_poll_mask(poll_mask)
}
fn io_uring_prep_timeout(sqe: ext:io_uring_sqe, ts: kernel_timespec, count: u32, flags: u32) {
    io_uring_prep_rw(ext:IORING_OP.TIMEOUT, sqe, -1, ts, 1, count)
    sqe.rw_flags = flags
}

fn io_uring_prep_poll_mask(poll_mask: u32) u32 {
    // TODO: if target == BIG ENDIAN CPU architecture
    // return __swahw32(poll_mask);
    // x86 / ARM (by default) = little-edian
    return poll_mask
}


fn io_uring_wait_cqe_nr(ring: ext:io_uring, cqe_ptr: *?ext:io_uring_cqe, wait_nr: u32) i32 {
     return ext:__io_uring_get_cqe(ring, cqe_ptr, 0, wait_nr, null)
}
fn io_uring_wait_cqe(ring: ext:io_uring, cqe_ptr: *?ext:io_uring_cqe) i32 {
    return io_uring_wait_cqe_nr(ring, cqe_ptr, 1)
}
fn io_uring_cqe_seen(ring: ext:io_uring, cqe: ext:io_uring_cqe) {
    io_uring_cq_advance(ring, 1);
}
// check if an io_uring completion event is available
fn io_uring_peek_cqe(ring: ext:io_uring, cqe_ptr: *?ext:io_uring_cqe) i32 {
    return io_uring_wait_cqe_nr(ring, cqe_ptr, 0)
}

fn io_uring_cq_advance(ring: ext:io_uring, nr: u32) {
    let cq = ring.cq.khead
    // cq[0] = cq[0] + nr
    atomic_store(cq[0], cq[0] + nr)
}

fn io_uring_cq_ready(ring: ext:io_uring) u32 {
    // return io_uring_smp_load_acquire(ring.cq.ktail) - ring.cq.khead[0]
    let cq = &ring.cq
    return atomic_load(cq.ktail[0]) - cq.khead[0]
}
#end

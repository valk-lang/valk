

#if OS == linux
header "sys" as sys

use coro

global uring_instance : ?sys:io_uring
global uring_unsubmitted : uint

value IORING_CQE_F_MORE (2)

fn uring() sys:io_uring {
    let ring = uring_instance
    if !isset(ring) {
        let r  = sys:io_uring{}
        let err = sys:io_uring_queue_init(1024, r, 0)
        uring_instance = r
        return r
    }
    return ring
}

fn uring_submit(ring: sys:io_uring) {
    let submitc = sys:io_uring_submit(ring)
    // if submitc != uring_unsubmitted : println("S: " + submitc + " / " + uring_unsubmitted)
    uring_unsubmitted -= submitc
}


fn sqe(coro: ?coro:Coro) sys:io_uring_sqe !full {
    if !isset(coro) : coro = coro:current_coro
    let ring = uring()
    let sqe = sys:io_uring_get_sqe(ring)
    if !isset(sqe) {
        uring_submit(ring)
        sqe = sys:io_uring_get_sqe(ring)
    }
    if !isset(sqe) : throw full
    // Set coro
    sqe.user_data = coro.@cast(u64)
    // Submit after x events
    uring_unsubmitted++
    if uring_unsubmitted > 128 : uring_submit(ring)
    //
    return sqe
}


fn io_uring_prep_rw(op: i32, sqe: sys:io_uring_sqe, fd: i32, addr: ?ptr, len: u32, offset: u64) {
    sqe.opcode = op.to(u8)
    sqe.flags = 0
    sqe.ioprio = 0
    sqe.fd = fd
    sqe.off = offset
    sqe.addr = addr.@cast(u64)
    sqe.len = len
    sqe.rw_flags = 0
    // sqe.user_data = 0
    sqe.buf_index = 0
    sqe.personality = 0
    sqe.file_index = 0
    sqe.pad2 = { 0 ... }
}

struct kernel_timespec {
    tv_sec: i64
    tv_nsec: i64
}
fn io_uring_prep_accept(sqe: sys:io_uring_sqe, fd: i32, addr: ?ptr, addrlen: uint, flags: i32) {
    io_uring_prep_rw(sys:IORING_OP.ACCEPT, sqe, fd, addr, 0, addrlen)
    sqe.rw_flags = flags
}

fn io_uring_prep_read(sqe: sys:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, offset: u64) {
    io_uring_prep_rw(sys:IORING_OP.READ, sqe, fd, buf, nbytes, offset)
}
fn io_uring_prep_recv(sqe: sys:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, flags: i32) {
    io_uring_prep_rw(sys:IORING_OP.RECV, sqe, fd, buf, nbytes, 0)
    sqe.rw_flags = flags
}

fn io_uring_prep_write(sqe: sys:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, offset: u64) {
    io_uring_prep_rw(sys:IORING_OP.WRITE, sqe, fd, buf, nbytes, offset)
}
fn io_uring_prep_send(sqe: sys:io_uring_sqe, fd: i32, buf: ptr, nbytes: u32, flags: i32) {
    io_uring_prep_rw(sys:IORING_OP.SEND, sqe, fd, buf, nbytes, 0)
    sqe.rw_flags = flags
}

fn io_uring_prep_poll_add(sqe: sys:io_uring_sqe, fd: i32, poll_mask: u32) {
    io_uring_prep_rw(sys:IORING_OP.POLL_ADD, sqe, fd, null, 0, 0)
    sqe.rw_flags = io_uring_prep_poll_mask(poll_mask)
}
fn io_uring_prep_poll_multi(sqe: sys:io_uring_sqe, fd: i32, poll_mask: u32) {
    io_uring_prep_rw(sys:IORING_OP.POLL_ADD, sqe, fd, null, 1, 0)
    sqe.rw_flags = io_uring_prep_poll_mask(poll_mask)
}
fn io_uring_prep_timeout(sqe: sys:io_uring_sqe, ts: kernel_timespec, count: u32, flags: u32) {
    io_uring_prep_rw(sys:IORING_OP.TIMEOUT, sqe, -1, ts, 1, count)
    sqe.rw_flags = flags
}

fn io_uring_prep_poll_mask(poll_mask: u32) u32 {
    // TODO: if target == BIG ENDIAN CPU architecture
    // return __swahw32(poll_mask);
    // x86 / ARM (by default) = little-edian
    return poll_mask
}


fn io_uring_wait_cqe_nr(ring: sys:io_uring, cqe_ptr: *?sys:io_uring_cqe, wait_nr: u32) i32 {
     return sys:__io_uring_get_cqe(ring, cqe_ptr, 0, wait_nr, null)
}
fn io_uring_wait_cqe(ring: sys:io_uring, cqe_ptr: *?sys:io_uring_cqe) i32 {
    return io_uring_wait_cqe_nr(ring, cqe_ptr, 1)
}
fn io_uring_cqe_seen(ring: sys:io_uring, cqe: sys:io_uring_cqe) {
    io_uring_cq_advance(ring, 1);
}
// check if an io_uring completion event is available
fn io_uring_peek_cqe(ring: sys:io_uring, cqe_ptr: *?sys:io_uring_cqe) i32 {
    return io_uring_wait_cqe_nr(ring, cqe_ptr, 0)
}

fn io_uring_cq_advance(ring: sys:io_uring, nr: u32) {
    let cq = ring.cq.khead
    // cq[0] = cq[0] + nr
    atomic_store(cq[0], cq[0] + nr)
}

fn io_uring_cq_ready(ring: sys:io_uring) u32 {
    // return io_uring_smp_load_acquire(ring.cq.ktail) - ring.cq.khead[0]
    let cq = &ring.cq
    return atomic_load(cq.ktail[0]) - cq.khead[0]
}
#end
